{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imblearn in c:\\users\\ifeoluwa david\\anaconda3\\lib\\site-packages (0.0)\n",
      "Requirement already satisfied: imbalanced-learn in c:\\users\\ifeoluwa david\\anaconda3\\lib\\site-packages (from imblearn) (0.6.2)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\ifeoluwa david\\anaconda3\\lib\\site-packages (from imbalanced-learn->imblearn) (0.14.1)\n",
      "Requirement already satisfied: numpy>=1.11 in c:\\users\\ifeoluwa david\\anaconda3\\lib\\site-packages (from imbalanced-learn->imblearn) (1.18.1)\n",
      "Requirement already satisfied: scikit-learn>=0.22 in c:\\users\\ifeoluwa david\\anaconda3\\lib\\site-packages (from imbalanced-learn->imblearn) (0.22)\n",
      "Requirement already satisfied: scipy>=0.17 in c:\\users\\ifeoluwa david\\anaconda3\\lib\\site-packages (from imbalanced-learn->imblearn) (1.4.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "# example of random undersampling to balance the class distribution\n",
    "!pip install imblearn\n",
    "import imblearn\n",
    "from collections import Counter\n",
    "from sklearn.datasets import make_classification\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "from time import time\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(dataset):\n",
    "\n",
    "    try:\n",
    "        \n",
    "        data = pd.read_csv('C:/Users/Ifeoluwa David/Dropbox/Lakehead Semester 4 (Fall 2019)/Degree Project/Project/'+dataset_name)\n",
    "        return data\n",
    "    \n",
    "    except IOError as e:\n",
    "        \n",
    "        print(\"Error: Unable to find the specified file!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_missing_data_percentage(data):\n",
    "    \n",
    "    # where mvp = missing value percentages\n",
    "    mvp = data.isnull().sum() * 100 / len(data)\n",
    "    mvp = pd.DataFrame({'Feature': data.columns,'Percentage': mvp})\n",
    "    \n",
    "    return mvp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_high_missing_data_columns(mvd, data):\n",
    "    \n",
    "    # Where \"mvd\" = missing value data\n",
    "    # Get names of indexes for which column missing data is over 50%\n",
    "    high_missing_data_cols = mvd[mvd['Percentage'] > 50].index\n",
    "\n",
    "    for col_name in range(len(high_missing_data_cols)):\n",
    "        del data[high_missing_data_cols[col_name]] # Delete rows from dataFrame\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_low_missing_data_columns(mvd, data):\n",
    "\n",
    "    # Get names of indexes for which column missing data is over 50%\n",
    "    low_missing_data_cols = mvd[mvd['Percentage'] != 0].index\n",
    "\n",
    "    for col_names in range(len(low_missing_data_cols)):\n",
    "\n",
    "        feature = data[low_missing_data_cols[col_names]].name\n",
    "\n",
    "        meanA = data[feature].mean()\n",
    "        data[feature] = data[feature].fillna(meanA)\n",
    "\n",
    "        meanB = data[feature].mean()\n",
    "        data[feature] = data[feature].fillna(meanB)\n",
    "    \n",
    "    return traindata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct columns with a mixture of numerical and textual values\n",
    "def correct_mixed_value_columns(data):\n",
    "    \n",
    "    data.dependency = data.dependency.replace({\"yes\": 1, \"no\": 0}).astype(np.float64)\n",
    "    data.edjefe = data.edjefe.replace({\"yes\": 1, \"no\": 0}).astype(np.float64)\n",
    "    data.edjefa = data.edjefa.replace({\"yes\": 1, \"no\": 0}).astype(np.float64)\n",
    "    #data.select_dtypes(include=[np.object]).head()\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_inconsistent_households(data):\n",
    "    \n",
    "    # Find households with inconsistent poverty classes among its members\n",
    "    contradictoryhouseholds = data.groupby('idhogar')['Target'].apply(lambda x: x.nunique() == 1)\n",
    "    contradictoryhouseholds = contradictoryhouseholds[contradictoryhouseholds != True]\n",
    "    #print(\"\\nBefore Correction: Number of households with inconsistent poverty classes: \"+str(contradictoryhouseholds.count())+\"\")\n",
    "    \n",
    "    # Iterate through each household and correct the inconsistent target values\n",
    "    for household in contradictoryhouseholds.index:\n",
    "        correctPovertyClass = int(data[(data['idhogar'] == household) & (data['parentesco1'] == 1.0)]['Target'])\n",
    "        data.loc[data['idhogar'] == household, 'Target'] = correctPovertyClass\n",
    "    \n",
    "    # Ensure households with inconsistent poverty classes don't exist\n",
    "    contradictoryhouseholds = data.groupby('idhogar')['Target'].apply(lambda x: x.nunique() == 1)\n",
    "    contradictoryhouseholds = contradictoryhouseholds[contradictoryhouseholds != True]\n",
    "    #print(\"\\nAfter Correction: Number of households with inconsistent poverty classes: \"+str(contradictoryhouseholds.count())+\"\")\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **************************************************************************************\n",
    "# Nested functions: Reduce the numbers columns with some ordinal representations\n",
    "\n",
    "def create_ordinal_features(data):\n",
    "    \n",
    "    #------------------------------ Create ordinal variables for \"wall condition\"\n",
    "\n",
    "    def wall_condition (row):\n",
    "       if row['epared1'] == 1:\n",
    "          return '1'\n",
    "       if row['epared2'] == 1:\n",
    "          return '2'\n",
    "       if row['epared3'] == 1:\n",
    "          return '3'\n",
    "       return '0'\n",
    "\n",
    "    data['wall_condition'] = data.apply (lambda row: wall_condition(row), axis=1)\n",
    "\n",
    "    del data['epared1']\n",
    "    del data['epared2']\n",
    "    del data['epared3']\n",
    "\n",
    "    #------------------------------ Create ordinal variables for \"roof condition\"\n",
    "\n",
    "    def roof_condition (row):\n",
    "       if row['etecho1'] == 1 :\n",
    "          return '1'\n",
    "       if row['etecho2'] == 1 :\n",
    "          return '2'\n",
    "       if row['etecho3'] == 1:\n",
    "          return '3'\n",
    "       return '0'\n",
    "\n",
    "    data['roof_condition'] = data.apply (lambda row: roof_condition(row), axis=1)\n",
    "\n",
    "    del data['etecho1']\n",
    "    del data['etecho2']\n",
    "    del data['etecho3']\n",
    "\n",
    "    #------------------------------ Create ordinal variables for \"floor condition\"\n",
    "\n",
    "    def floor_condition (row):\n",
    "       if row['eviv1'] == 1 :\n",
    "          return '1'\n",
    "       if row['eviv2'] == 1 :\n",
    "          return '2'\n",
    "       if row['eviv3'] == 1:\n",
    "          return '3'\n",
    "       return '0'\n",
    "\n",
    "    data['floor_condition'] = data.apply (lambda row: floor_condition(row), axis=1)\n",
    "\n",
    "    del data['eviv1']\n",
    "    del data['eviv2']\n",
    "    del data['eviv3']\n",
    "\n",
    "    #------------------------------ Create ordinal variables for \"Education Achieved\"\n",
    "\n",
    "    def education_achieved (row):\n",
    "       if row['instlevel1'] == 1 :\n",
    "          return '1'\n",
    "       if row['instlevel2'] == 1 :\n",
    "          return '2'\n",
    "       if row['instlevel3'] == 1:\n",
    "          return '3'\n",
    "       if row['instlevel4'] == 1 :\n",
    "          return '4'\n",
    "       if row['instlevel5'] == 1 :\n",
    "          return '5'\n",
    "       if row['instlevel6'] == 1:\n",
    "          return '6'\n",
    "       if row['instlevel7'] == 1 :\n",
    "          return '7'\n",
    "       if row['instlevel8'] == 1:\n",
    "          return '8'\n",
    "       if row['instlevel9'] == 1:\n",
    "          return '9'\n",
    "       return '0'\n",
    "\n",
    "    data['education_achieved'] = data.apply (lambda row: education_achieved(row), axis=1)\n",
    "\n",
    "    del data['instlevel1']\n",
    "    del data['instlevel2']\n",
    "    del data['instlevel3']\n",
    "    del data['instlevel4']\n",
    "    del data['instlevel5']\n",
    "    del data['instlevel6']\n",
    "    del data['instlevel7']\n",
    "    del data['instlevel8']\n",
    "    del data['instlevel9']\n",
    "    \n",
    "    data['wall_condition'] = data['wall_condition'].astype(np.float64)\n",
    "    data['roof_condition'] = data['roof_condition'].astype(np.float64)\n",
    "    data['floor_condition'] = data['floor_condition'].astype(np.float64)\n",
    "    data['education_achieved'] = data['education_achieved'].astype(np.float64)\n",
    "    \n",
    "    return data\n",
    "\n",
    "# **************************************************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_one_value_columns(data):\n",
    "    \n",
    "    # Drop columns with only 1 unique value.\n",
    "    for column in data.columns:\n",
    "        if len(data[column].unique()) == 1:\n",
    "            #print(traindata[column].name)\n",
    "            data.drop(column,inplace=True,axis=1)\n",
    "            \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_highly_correlated_features(data):\n",
    "    \n",
    "    # Drop columns with over 80% correlation\n",
    "    threshold = 0.80\n",
    "    corr_matrix = data.corr()\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "    to_drop = [column for column in upper.columns if any(abs(upper[column]) > threshold)]\n",
    "    data = data.drop(columns = to_drop)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_inputs_and_outputs(data):\n",
    "    \n",
    "    # Prepare & save the inputs and outputs features\n",
    "    features = data.drop(['Target','Id','idhogar'], axis = 1)\n",
    "    labels = data[['Target']]\n",
    "    \n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_scale(features):\n",
    "    \n",
    "    # Feature Scaling: Standardization\n",
    "    cols = features.columns\n",
    "    sc = StandardScaler()\n",
    "    features = sc.fit_transform(features)\n",
    "    features = pd.DataFrame(features, columns=cols)\n",
    "    \n",
    "    return features , sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minmax_normalize(features):\n",
    "    \n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    cols = features.columns\n",
    "    features_np_array = features.values # returns a numpy array\n",
    "    features = min_max_scaler.fit_transform(features_np_array)\n",
    "    features = pd.DataFrame(features, columns=cols)\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selectkbestfeatures(features, labels):\n",
    "    \n",
    "    # Apply SelectKBest class to extract top 10 best features\n",
    "    bestfeatures = SelectKBest(score_func=f_classif, k=25)\n",
    "    bestfeatures.fit(features, labels)\n",
    "    \n",
    "    # Get columns to keep and create new dataframe with those only\n",
    "    cols = bestfeatures.get_support(indices=True)\n",
    "    features = features.iloc[:,cols]\n",
    "    \n",
    "    features.to_csv('train_features.csv')\n",
    "    labels.to_csv('train_labels.csv')\n",
    "    data = features.join(labels)\n",
    "    data.to_csv('train_features_and_labels.csv')\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(features, labels):\n",
    "    \n",
    "    # Data Splitting: 60% for training, 20% for validation and 20% for testing.\n",
    "    X_train, X_test, Y_train, y_test = train_test_split(features, labels, test_size=0.4)\n",
    "    X_validation, X_test, Y_validation, y_test = train_test_split(X_test, y_test, test_size=0.5)\n",
    "    \n",
    "    return X_train, Y_train, X_test, y_test, X_validation, Y_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_and_fit_model(train_inputs, train_outputs, selection):\n",
    "    \n",
    "    if str(selection) == '1':\n",
    "        #print(\"Random Forest\")\n",
    "        algorithm = RandomForestClassifier()\n",
    "        parameters = {\n",
    "            'n_estimators': [20, 30, 40, 50],\n",
    "            'max_depth': [5, 10, 20, 25, 30, 35, 40, 45, 50]\n",
    "        }\n",
    "        \n",
    "    elif str(selection) == '2':\n",
    "        #print(\"Logistic Regression\")\n",
    "        algorithm = LogisticRegression()\n",
    "        parameters = {\n",
    "            'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "        }\n",
    "        \n",
    "    elif str(selection) == '3':\n",
    "        #print(\"Multi Layer Perceptron\")\n",
    "        algorithm = MLPClassifier()\n",
    "        parameters = {\n",
    "            'hidden_layer_sizes': [(10,), (50,), (100,)],\n",
    "            'activation': ['relu', 'tanh', 'logistic'],\n",
    "            'learning_rate': ['constant', 'invscaling', 'adaptive']\n",
    "        }\n",
    "        \n",
    "    elif str(selection) == '4':\n",
    "        #print(\"Support Vector Machines\")\n",
    "        algorithm = SVC()\n",
    "        parameters = {\n",
    "            'kernel': ['linear', 'rbf'],\n",
    "            'C': [0.1, 1, 10]\n",
    "        }\n",
    "        \n",
    "    elif str(selection) == '5':\n",
    "        #print(\"Gradient Boosting\")\n",
    "        algorithm = GradientBoostingClassifier()\n",
    "        parameters = {\n",
    "            'n_estimators': [5, 50, 250, 500],\n",
    "            'max_depth': [1, 3, 5, 7, 9],\n",
    "            'learning_rate': [0.01, 0.1, 1, 10, 100]\n",
    "        }\n",
    "            \n",
    "    else:\n",
    "        #print(\"Random Forest\")\n",
    "        algorithm = RandomForestClassifier()\n",
    "        parameters = {\n",
    "            'n_estimators': [20, 30, 40, 50],\n",
    "            'max_depth': [5, 10, 20, 25, 30, 35, 40, 45, 50]\n",
    "        }\n",
    "\n",
    "    cv = GridSearchCV(algorithm, parameters, cv=5)\n",
    "    cv.fit(train_inputs, train_outputs.values.ravel())\n",
    "    \n",
    "    return cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(name, model, features, labels):\n",
    "    \n",
    "    start = time()\n",
    "    pred = model.predict(features)\n",
    "    end = time()\n",
    "    \n",
    "    # Print the confusion matrix\n",
    "    print(metrics.confusion_matrix(labels, pred))\n",
    "\n",
    "    # Print the precision and recall, among other metrics\n",
    "    print(metrics.classification_report(labels, pred, digits=3))\n",
    "    \n",
    "    #print(name+\" Accuracy - \"+str(round(accuracy_score(labels, pred), 3) * 100)+\"%\")\n",
    "    #print(name+\" Precision - \"+str(round(precision_score(labels, pred, average='micro'), 3) * 100)+\"%\")\n",
    "    #print(name+\" Recall - \"+str(round(recall_score(labels, pred, average='micro'), 3) * 100)+\"%\")\n",
    "    #print(name+\" F1 Score - \"+str(round(f1_score(labels, pred, average='micro'), 3) * 100)+\"%\")\n",
    "    #print(name+\" Latency - \"+str(round((end - start) * 1000, 1))+\"ms \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def request_inputs():\n",
    "    \n",
    "    # 1. Request the dataset\n",
    "    dataset = input(\"Enter Your Dataset Name: \")\n",
    "    #dataset_name = sys.argv[1]\n",
    "\n",
    "    # 2. Request an algorithm\n",
    "    print(\"\\nEnter 1 for Random Forest\")\n",
    "    print(\"Enter 2 for Logistic Regression\")\n",
    "    print(\"Enter 3 for Multi Layer Perceptron\")\n",
    "    print(\"Enter 4 for Support Vector Machine\")\n",
    "    print(\"Enter 5 for Gradient Boosting\")\n",
    "    number = input(\"\\nSelect an algorithm from the menu options given: \")\n",
    "    #algorithm_number = sys.argv[2]\n",
    "    \n",
    "    return dataset, number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, scaler):\n",
    "    \n",
    "    model_name = input(\"\\nEnter New Model Name: \")\n",
    "    #model_name = sys.argv[3]\n",
    "    \n",
    "    joblib.dump([model.best_estimator_, scaler], model_name)\n",
    "    print(\"\\nModel Saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter Your Dataset Name: train.csv\n",
      "\n",
      "Enter 1 for Random Forest\n",
      "Enter 2 for Logistic Regression\n",
      "Enter 3 for Multi Layer Perceptron\n",
      "Enter 4 for Support Vector Machine\n",
      "Enter 5 for Gradient Boosting\n",
      "\n",
      "Select an algorithm from the menu options given: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ifeoluwa David\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:744: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 113  132    8  197]\n",
      " [  41  370    4  543]\n",
      " [  11  155   22  545]\n",
      " [  15  180   19 3379]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1      0.628     0.251     0.359       450\n",
      "           2      0.442     0.386     0.412       958\n",
      "           3      0.415     0.030     0.056       733\n",
      "           4      0.724     0.940     0.818      3593\n",
      "\n",
      "    accuracy                          0.677      5734\n",
      "   macro avg      0.552     0.402     0.411      5734\n",
      "weighted avg      0.630     0.677     0.617      5734\n",
      "\n",
      "[[  33   61    2   71]\n",
      " [  16   97    2  173]\n",
      " [   9   74    8  166]\n",
      " [   8   55    5 1131]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1      0.500     0.198     0.283       167\n",
      "           2      0.338     0.337     0.337       288\n",
      "           3      0.471     0.031     0.058       257\n",
      "           4      0.734     0.943     0.826      1199\n",
      "\n",
      "    accuracy                          0.664      1911\n",
      "   macro avg      0.511     0.377     0.376      1911\n",
      "weighted avg      0.618     0.664     0.601      1911\n",
      "\n",
      "[[  36   46    6   69]\n",
      " [  24  120    2  166]\n",
      " [   4   51    7  169]\n",
      " [   4   70    3 1135]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1      0.529     0.229     0.320       157\n",
      "           2      0.418     0.385     0.401       312\n",
      "           3      0.389     0.030     0.056       231\n",
      "           4      0.737     0.936     0.825      1212\n",
      "\n",
      "    accuracy                          0.679      1912\n",
      "   macro avg      0.518     0.395     0.401      1912\n",
      "weighted avg      0.626     0.679     0.622      1912\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2. Get user inputs\n",
    "dataset_name, algorithm_number = request_inputs()\n",
    "\n",
    "# 3. Load the Dataset\n",
    "traindata = load_dataset(dataset_name)\n",
    "\n",
    "# 4. Handle the Missing Data\n",
    "missing_value_data = get_missing_data_percentage(traindata)\n",
    "traindata = drop_high_missing_data_columns(missing_value_data, traindata)\n",
    "missing_value_data = get_missing_data_percentage(traindata)\n",
    "traindata = impute_low_missing_data_columns(missing_value_data,  traindata)\n",
    "\n",
    "# 5. Correct any inconsistencies in the data\n",
    "traindata = correct_mixed_value_columns(traindata)\n",
    "traindata = correct_inconsistent_households(traindata)\n",
    "\n",
    "# 6. Feature Extraction & Selection\n",
    "traindata = create_ordinal_features(traindata)\n",
    "traindata = drop_one_value_columns(traindata)\n",
    "traindata = drop_highly_correlated_features(traindata)\n",
    "\n",
    "# 7. Feature & Label Separation\n",
    "train_features, train_labels = prepare_inputs_and_outputs(traindata)\n",
    "\n",
    "# 8. Feature Selection (1)\n",
    "train_features = selectkbestfeatures(train_features, train_labels)\n",
    "\n",
    "# 9. Data Splitting\n",
    "X_train, Y_train, X_test, y_test, X_validation, Y_validation = split_data(train_features, train_labels)\n",
    "\n",
    "# 10. Feature Scaling (1)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_validation = scaler.transform(X_validation)\n",
    "X_test = scaler.transform(X_test)\n",
    "#train_features = minmax_normalize(train_features)\n",
    "\n",
    "# 11. Model Fitting\n",
    "cv_model = select_and_fit_model(X_train, Y_train, algorithm_number)\n",
    "\n",
    "# 12. Model Evaluation\n",
    "evaluate_model('Train Set', cv_model, X_train, Y_train)\n",
    "evaluate_model('Validation Set', cv_model, X_validation, Y_validation)\n",
    "evaluate_model('Test Set', cv_model, X_test, y_test)\n",
    "\n",
    "# 13. Save Model\n",
    "#save_model(cv_model, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "classes = [1,2,3,4]\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "titles_options = [(\"Confusion matrix, without normalization\", None),(\"Normalized confusion matrix\", 'true')]\n",
    "for title, normalize in titles_options:\n",
    "    disp = plot_confusion_matrix(cv_model, X_test, y_test,\n",
    "                                 display_labels=classes,\n",
    "                                 cmap=plt.cm.Blues,\n",
    "                                 normalize=normalize)\n",
    "    disp.ax_.set_title(title)\n",
    "\n",
    "    #print(title)\n",
    "    #print(disp.confusion_matrix)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 469    0    0    0]\n",
      " [   0  934    0    0]\n",
      " [   0    0  740    0]\n",
      " [   0    0    0 3591]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1      1.000     1.000     1.000       469\n",
      "           2      1.000     1.000     1.000       934\n",
      "           3      1.000     1.000     1.000       740\n",
      "           4      1.000     1.000     1.000      3591\n",
      "\n",
      "    accuracy                          1.000      5734\n",
      "   macro avg      1.000     1.000     1.000      5734\n",
      "weighted avg      1.000     1.000     1.000      5734\n",
      "\n",
      "[[ 126    3    4   19]\n",
      " [   0  276    2   39]\n",
      " [   3    4  182   33]\n",
      " [   4   11    9 1196]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1      0.947     0.829     0.884       152\n",
      "           2      0.939     0.871     0.903       317\n",
      "           3      0.924     0.820     0.869       222\n",
      "           4      0.929     0.980     0.954      1220\n",
      "\n",
      "    accuracy                          0.931      1911\n",
      "   macro avg      0.935     0.875     0.903      1911\n",
      "weighted avg      0.932     0.931     0.930      1911\n",
      "\n",
      "[[ 123    4    3   23]\n",
      " [   0  257    1   49]\n",
      " [   3   10  207   39]\n",
      " [   2   12   10 1169]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1      0.961     0.804     0.875       153\n",
      "           2      0.908     0.837     0.871       307\n",
      "           3      0.937     0.799     0.863       259\n",
      "           4      0.913     0.980     0.945      1193\n",
      "\n",
      "    accuracy                          0.918      1912\n",
      "   macro avg      0.930     0.855     0.889      1912\n",
      "weighted avg      0.919     0.918     0.917      1912\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_model('Train Set', cv_model, X_train, Y_train)\n",
    "evaluate_model('Validation Set', cv_model, X_validation, Y_validation)\n",
    "evaluate_model('Test Set', cv_model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    6004\n",
       "2    1558\n",
       "3    1221\n",
       "1     774\n",
       "Name: Target, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindata['Target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
